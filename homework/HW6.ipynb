{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 0\n",
    "\n",
    "It is highly recommended that you work with your group to fully complete the recent Discussion assignments related to the Palmer Penguins data set, as these will directly help with your project. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 1\n",
    "\n",
    "Gapminder is a foundation, based on Sweden, that aims to enhance basic awareness of basic facts about the socioeconomic global world. As part of their efforts, they collect detailed statistics on life expectancy, population, and GDP, sometimes going back over many years. \n",
    "\n",
    "Here, we'll work with an excerpt of the Gapminder data. This excerpt has been packaged up and made available via Jenny Bryan's [`gapminder` repo](https://github.com/jennybc/gapminder) on Github. \n",
    "\n",
    "Run the code below to retrieve the data and take a look. As usual, you can also directly download the data by pasting the url into your browswer, saving the file, and reading it in locally via `pandas.read_csv`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "        \n",
    "url = \"https://philchodrow.github.io/PIC16A/datasets/gapminder.csv\"\n",
    "gapminder = pd.read_csv(url)\n",
    "gapminder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the `gapminder` data to create the folowing visualization. Here, each trendline corresponds to a distinct country, which in turn lies on the stated continent. \n",
    "\n",
    "<figure class=\"image\" style=\"width:100%\">\n",
    "  <img src=\"https://raw.githubusercontent.com/PhilChodrow/PIC16A/master/homework/gapminder_p1.png\" alt=\"\">\n",
    "  <figcaption><i></i></figcaption>\n",
    "</figure>\n",
    "\n",
    "You should achieve this result **without for-loops**, and also without manually creating the plot on each axis. You may find it useful to define additional data structures, such as dictionaries, that assign colors or axis indices to each continent. Feel free to modify aesthetic details of the plots, such as the colors. \n",
    "\n",
    "**Hint**: Try `df.groupby([\"country\"]).apply(f)`, where `f` is some very smart function that you have figured out. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your solution here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 2\n",
    "\n",
    "In this problem, you will create several attractive visualizations of the `gapminder` data set using the [Seaborn package](https://seaborn.pydata.org/). Seaborn is a high-level interface to Matplotlib specially designed for working with Pandas data frames. Seaborn makes it relatively easy to create complex, multifaceted data graphics. The drawback is that it can sometimes be more difficult to exercise control over figure details, such as axis labels or figure size.\n",
    "\n",
    "Part of the learning objective for this problem is for you to **practice learning from documentation and examples**. I have given you some pointers in each problem, but it will be up to you to fill in the blanks. \n",
    "\n",
    "Import the `seaborn` package with the line `import seaborn as sns`. Then, run the code block below to create a scatterplot of `gdpPercap` (gross domestic product per capita) against `lifeExp` (life expectancy), with different colors for each continent. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "fgrid = sns.relplot(x = \"gdpPercap\", \n",
    "                    y = \"lifeExp\", \n",
    "                    hue = \"continent\", \n",
    "                    data = gapminder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result of the above code is that `fgrid` is now a `FacetGrid` object (we'll see why it's called that in a minute). However, under the hood, we are still dealing with Matplotlib. To retrieve the individual plotting axis, we can access it from the `fgrid.axes` attribute, which is a 2d Numpy array containing the axes. For example, to make the horizontal axis logarithmic and labels, we can do this: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fgrid = sns.relplot(x = \"gdpPercap\", \n",
    "                    y = \"lifeExp\", \n",
    "                    hue = \"continent\", \n",
    "                    data = gapminder)\n",
    "\n",
    "fgrid.axes[0][0].semilogx()\n",
    "\n",
    "labels = fgrid.axes[0][0].set(xlabel = \"GDP per capita\", \n",
    "                              ylabel = \"Life Expectancy\",\n",
    "                              title  = \"Impact of Economic Output on Life Expectancy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not bad! Note that seaborn has effectively looped over the different continents, without us needing to use `for`-loops or `apply`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part A\n",
    "\n",
    "Create the following plot using Seaborn: \n",
    "\n",
    "<figure class=\"image\" style=\"width:100%\">\n",
    "  <img src=\"https://philchodrow.github.io/PIC16A/homework/seaborn-example-1.png\n",
    "\" alt=\"\" width=\"800px\">\n",
    "</figure>\n",
    "\n",
    "In this figure, each point represents a country, and the size of the point corresponds to the population. \n",
    "\n",
    "In this part and the next, it is not necessary that your plot *precisely* match mine -- feel free to modify the sizes of labels, change the color scheme, etc. \n",
    "\n",
    "#### Hints\n",
    "\n",
    "- You should use [`sns.relplot`](https://seaborn.pydata.org/generated/seaborn.relplot.html) to create the figure. \n",
    "- Look in the [Seaborn Gallery](https://seaborn.pydata.org/examples/index.html) for examples illustrating how to create a multi-panel figure without using `plt.subplots()`. \n",
    "- The size of the figure is controlled by the `height` and `aspect` ratio arguments to `relplot`. I used `height = 2` and `aspect = 1.3`, but other choices are also fine. \n",
    "- When using column wrapping to create the `3 x 4` axis array, the resulting `fgrid.axes` array is no longer 2-dimensional, but 1-dimensional. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part B\n",
    "\n",
    "Create the following plot using Seaborn:\n",
    "\n",
    "<figure class=\"image\" style=\"width:100%\">\n",
    "  <img src=\"https://philchodrow.github.io/PIC16A/homework/seaborn-example-2.png\n",
    "\" alt=\"\" width=\"500px\">\n",
    "</figure>\n",
    "\n",
    "#### Hints\n",
    "\n",
    "- The easiest way is to again use `relplot`. This function defaults to producing scatterplots, but if you check the [gallery](https://seaborn.pydata.org/examples/index.html) you'll find examples of how to specify keywords that create line charts instead. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part C\n",
    "\n",
    "Create the following (very long) plot using Seaborn:\n",
    "\n",
    "<figure class=\"image\" style=\"width:100%\">\n",
    "  <img src=\"https://philchodrow.github.io/PIC16A/homework/seaborn-example-3.png\n",
    "\" alt=\"\" width=\"300px\">\n",
    "</figure>\n",
    "\n",
    "#### Hints\n",
    "\n",
    "- Use `sns.barplot`. \n",
    "- Pick a single year of the `gapminder` data frame to show. In the example, I showed data from 2002 only. \n",
    "- To sort the bars in ascending order by continent, you should sort the data frame that you pass to `sns.barplot`. \n",
    "- The argument `dodge = False` is a helpful keyword that will keep your bars centered. \n",
    "- I used a different color palette just to shake things up a little. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part D\n",
    "\n",
    "Create the one last plot using Seaborn:\n",
    "\n",
    "<figure class=\"image\" style=\"width:100%\">\n",
    "  <img src=\"https://philchodrow.github.io/PIC16A/homework/seaborn-example-4.png\n",
    "\" alt=\"\" width=\"500px\">\n",
    "</figure>\n",
    "\n",
    "#### Hints\n",
    "\n",
    "- This is a boxplot. \n",
    "- You should again pick a single year of the `gapminder` data frame to show. In the example, I show data from 2002 only. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 3\n",
    "\n",
    "In our first lecture on machine learning, we did linear regression \"by hand.\" In this problem, we will similarly perform logistic regression \"by hand.\" This homework problem is closely parallel to the lecture, and so you might want to have the [notes](https://nbviewer.jupyter.org/github/PhilChodrow/PIC16A/blob/master/content/ML/ML_2.ipynb) handy.  \n",
    "\n",
    "Although logistic regression is a relatively simple model, it is a foundation for many modern deep neural nets. Additioanlly, logistic regression itself is still often used in applied contexts even now. \n",
    "\n",
    "<blockquote class=\"twitter-tweet\"><p lang=\"en\" dir=\"ltr\">&quot;When we raise money itâ€™s AI, when we hire it&#39;s machine learning, and when we do the work it&#39;s logistic regression.&quot;<br><br>(I&#39;m not sure who came up with this but it&#39;s a gem ðŸ’Ž)</p>&mdash; Dr. Daniela Witten (@daniela_witten) <a href=\"https://twitter.com/daniela_witten/status/1177294449702928384?ref_src=twsrc%5Etfw\">September 26, 2019</a></blockquote> <script async src=\"https://platform.twitter.com/widgets.js\" charset=\"utf-8\"></script>\n",
    "\n",
    "Whereas linear regression is often used to measure arbitrary quantities like GDP or happiness scores, logistic regression is primarily used to estimate *probabilities*. For example, we might use logistic regression to estimate the probability of a passenger surviving the Titanic crash, a borrower defaulting on a loan, or an email being spam.\n",
    "\n",
    "For concreteness, let's say that we are considering the latter case. Suppose that we wish to model the probability that an email is spam as a function of the proportion of flag words (like \"investment\", \"capital\", \"bank\", \"account\", etc.) in the email's body text. Call this proportion $x$. $x$ is then a variable between $0$ and $1$. \n",
    "\n",
    "In logistic regression, we suppose that the probability $p$ that an email is spam has the form \n",
    "\n",
    "$$p = \\frac{1}{1+e^{-ax - b}}\\;,$$\n",
    "\n",
    "where $a$ and $b$ are again parameters. Let's see how this looks. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run this block\n",
    "\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "n_points = 1000\n",
    "\n",
    "a = 10\n",
    "b = -5\n",
    "\n",
    "x = np.sort(np.random.rand(n_points))\n",
    "p = 1/(1+np.exp(-a*x - b))\n",
    "\n",
    "fig, ax = plt.subplots(1)\n",
    "ax.plot(x, p, color = \"black\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As usual, in practice we don't have access to the true function telling us the probability that an email is spam. Instead, we have access to data telling us whether or not the email really IS spam. We can model this situation by flipping a biased coin for each email, with the probability of heads determined by the modeled probability. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run this block\n",
    "y = 1.0*(np.random.rand(n_points) < p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A value of 1 indicates that the email is indeed spam, while a value of 0 indicates that the email is not spam. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run this block\n",
    "ax.scatter(x, y,  alpha = 0.5)\n",
    "fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that there are more spam emails where the model gives a high probability, and fewer where the model gives a lower probability. However, there may be some non-spam emails with even high probability -- sometimes we get legitimate emails about bank accounts, investments, etc.  \n",
    "\n",
    "Of course, we don't have access to the true model, so our practical situation looks more like this: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run this block\n",
    "fig, ax = plt.subplots(1)\n",
    "ax.scatter(x, y, alpha = 0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We would like to use logistic regression to try to recover something close to the true model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part A\n",
    "\n",
    "Write the model function `f`. The arguments of `f` should be the predictor variables `x` and the parameters `a` and `b`. The output of `f` should be the spam probabilities under the logistic model (see equation above) for these data. Use `numpy` tools, without `for`-loops. If you scan the above code carefully, you'll observe that most of this code is already written for you. \n",
    "\n",
    "This is a simple function, but **please add a short docstring indicating** what kinds of input it accepts and how to interpret the output. \n",
    "\n",
    "Comments are necessary only if your function body exceeds one line. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your solution\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part B\n",
    "\n",
    "Plot 10 candidate models against the data, using randomly chosen values of `a` between 5 and 15 and randomly chosen values of `b` between -2.5 and -7.5. Your plot should resemble in certain respects the third plot in [these lecture notes](https://nbviewer.jupyter.org/github/PhilChodrow/PIC16A/blob/master/content/ML/ML_2.ipynb). \n",
    "\n",
    "Comments are not necessary in this part. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your solution here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part C\n",
    "\n",
    "The *loss function* most commonly used in logistic regression is the *negative cross-entropy*. The negative cross-entropy of the `i`th observation is \n",
    "\n",
    "$$-\\left[y_i \\log \\hat{p}_i + (1-y_i)\\log(1-\\hat{p}_i)\\right]$$\n",
    "\n",
    "where $y_i \\in \\{0,1\\}$ is the `i`th entry of the target data and $\\hat{p}_i$ is the model's estimated probability that $y_i = 1$. The negative cross-entropy of the entire data set is the sum of the negative cross-entropies for each individual observation. \n",
    "\n",
    "Write a function that computes the negative cross entropy as a function of `x`, `y`, `a`, and `b`. This can be done in no more than two lines using `numpy`, without `for`-loops. Don't forget which logarithm is \\#BestLogarithm.  \n",
    "\n",
    "As in Part B, please write a short docstring describing what your function does and what inputs it accepts. Comments are necessary only if your function body exceeds two lines. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your solution here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part D\n",
    "\n",
    "On a single axis, plot 100 distinct models (using the code that you wrote in) in Part B. Highlight the one with the lowest negative cross entropy in a different color -- say, red. Compare the best values of `a` and `b` that you found to the true values, which were `a = 10` and `b = -5`. Are you close? \n",
    "\n",
    "The plot you produce should resemble, in some respects, the fifth plot in [these lecture notes](https://nbviewer.jupyter.org/github/PhilChodrow/PIC16A/blob/master/content/ML/ML_2.ipynb). \n",
    "\n",
    "It is not necessary to write comments in this part. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your solution here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is not required to use `scipy.optimize` to more accurately estimate `a` and `b` for this homework assignment, but you are  free to do so if you wish. You may then use the optimal estimates in the following part. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part E\n",
    "\n",
    "In classification tasks, we evaluate not just the standard loss function, but also the *accuracy* -- how often does the model correctly classify the data? Let's say that the model classifies an email as spam according to the following rule: \n",
    "\n",
    "1. If $\\hat{p}_i$ (the model probability plotted above) is larger than $c$, classify the email as spam. \n",
    "2. If $\\hat{p}_i$ is less than or equal to $c$, classify the email as not-spam. \n",
    "\n",
    "Write a function called `positive_rates` which accepts the following arguments: \n",
    "\n",
    "1. The data, `x` and `y`. \n",
    "2. The best parameters `best_a` and `best_b`. \n",
    "3. A threshold `c` between 0 and 1. \n",
    "\n",
    "This function should output two numbers. The first of these is *false positive rate*: the proportion of non-spam emails that the model incorrectly labels as spam. The second is the *true positive rate*: the proportion of spam emails that the model correctly labels as spam. \n",
    "\n",
    "For example: \n",
    "\n",
    "```python \n",
    "positive_rates(x, y, best_a, best_b, c = 0.5)\n",
    "```\n",
    "```\n",
    "(0.05058365758754864, 0.7469135802469136)\n",
    "```\n",
    "\n",
    "**Note**: due to randomization, your numerical output may be  different. \n",
    "\n",
    "Please write a descriptive docstring for your function. Comments are necessary only if your function body exceeds five lines. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your solution here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# demonstrate your function here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part F\n",
    "\n",
    "Plot the *receiver operating characteristic* (ROC) curve for the logistic model with parameters `best_a` and `best_b`. The ROC curve is the plot of the `false_positive` rate (on the horizontal axis) against the `true_positive` rate (on the vertical axis) as the threshold `c` is allowed to vary. Additionally, plot a diagonal line (\"the line of equality\") between the points (0,0) and (1,1). Your ROC curve should lie noticeably above the line of equality. Plot your curves in different colors and add a legend to help your reader understand the plot. \n",
    "\n",
    "It is ok to use `for`-loops and list comprehensions in this part. \n",
    "\n",
    "Your result should look something like this, although it won't be exactly the same due to randomness. \n",
    "\n",
    "<figure class=\"image\" style=\"width:60%\">\n",
    "  <img src=\"https://raw.githubusercontent.com/PhilChodrow/PIC16A/master/homework/roc-example.png\" alt=\"The horizontal and vertical axes are the false positive and true positive rates. A diagonal black line goes from the bottom left to the top right. A red curve above the black line indicates the classifier performance.\">\n",
    "  <figcaption><i></i></figcaption>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generally speaking, a \"good\" classifier is one that can reach the closest to the top-left corner of the ROC diagram. This is a classifier that can achieve a high rate of true positives, while keeping a low rate of false positives. There are various ways to measure how \"good\" an ROC curve is, which are beyond our present scope. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
