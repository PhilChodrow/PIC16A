{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Online Data\n",
    "\n",
    "Webscraping is the activity of downloading, manipulating, and using information obtained online. Webscraping can get very complicated, and we won't do much in this course. This set of lecture notes can help you get started on the basics. We'll look into this a bit more when we get to regular expressions in a few lectures. \n",
    "\n",
    "## Downloading Files\n",
    "\n",
    "There are several modules for downloading files from the internet. We'll use `urllib`: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conveniently, my one of my colleagues has a copy of the Palmer Penguins dataset on his website"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#I typed this so you don't have to!\n",
    "url = \"https://philchodrow.github.io/PIC16A/content/IO_and_modules/IO/palmer_penguins.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's read in the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This part I am making you type at least once :("
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "aving run this code, you can check in your file explorer that a file called `interweb_penguins.csv` now lives in the same directory as this notebook. We used the somewhat unusual flag `\"wb\"` to `open()` in order to indicate that we need to write a *binary* file, rather than the usual text file. This is because `to_write, the return value of `filedata.read()`, is by default binary data. We might ask you in assignments to use this pattern, but you we won't evaluate you on it in any timed or closed-book contexts. \n",
    "\n",
    "The module `wget` is another popular tool for downloading files from the internet. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data from Websites\n",
    "\n",
    "Often, we want to access the contents of a webpage. In this case, the request.urlopen submodule of urllib can help us easily access the contents of a desired URL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is a website the url of my old website."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "url=\"https://www.math.purdue.edu/~mperlmut/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code reads in the html code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at html_bits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not too nice! Instad, lets add a decode line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Better!__ Now, lets say we want to extract all of the links on this website\n",
    "\n",
    "We will do this using regular expressions. We will learn more about regular expressions later in the course, but since we haven't covered them yet, __you have my permission to stop typing for the rest of the video.__ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "urls=re.findall(r'href=[\\'\"]?([^\\'\">]+)',html)\n",
    "\n",
    "urls\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we want to only look at urls containing http, we can use a list constructor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "[line for line in urls if \"http\" in line ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
