{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Selection\n",
    "\n",
    "In the last few lectures, we learned how to use hold-out \"test\" sets and cross-validation to gain appropriate estimates of a model's performance on unseen data. There, the focus was on choosing a good \"complexity\" parameter, such as the depth of a decision tree. In this lecture, we'll instead show how to use cross-validation to get an estimate of which columns in the data should or should not be included in a model. It's very common in practice that not all columns will be used in the best model, and many, many machine learning reseachers devote their careers to studying the problem of how to intelligently and automatically choose only the most relevant columns for models. In the literature, this problem is usually called *feature selection*. In this lecture, we'll take a quick look at how feature selection can improve model performance. \n",
    "\n",
    "For this demonstration, we'll switch from decision trees to logistic regression. Logistic regression is a form of regression modeling well-suited for predicting probabilities and class labels. \n",
    "\n",
    "Let's begin by running some familiar blocks of code, in which we load our core libraries, read in the data, split the data, and clean the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#standard imports\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "#read in data\n",
    "titanic=pd.read_csv(\"titanic.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to divide the data into a training and testing sets and prepocess it by changing male/female to 1/0 and dropping the name column. This code is nearly exactly the same as earlier videos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "np.random.seed(1111)\n",
    "train, test = train_test_split(titanic, test_size = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "def prep_titanic_data(data_df):\n",
    "    df = data_df.copy()\n",
    "    le = preprocessing.LabelEncoder()\n",
    "    df['Sex'] = le.fit_transform(df['Sex'])\n",
    "    df = df.drop(['Name'], axis = 1)\n",
    "    \n",
    "    X = df.drop(['Survived'], axis = 1)\n",
    "    y = df['Survived']\n",
    "        \n",
    "    return(X, y)\n",
    "\n",
    "X_train, y_train = prep_titanic_data(train)\n",
    "X_test,  y_test  = prep_titanic_data(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using logistic regression is almost exactly the same as using the decision tree classifier. Let's go ahead and use cross-validation to estimate the predictive performance of the model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7940365597842375"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "LR=LogisticRegression()\n",
    "cross_val_score(LR,X_train,y_train,cv=5).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Is this the best we can do? If you've studied logistic regression before, you may know that using lots of columns doesn't always help -- due to *multicollinearity*, the model's predictive performance can actually suffer. This is actually another aspect of *overfitting*. Adding more columns makes the model more flexible, and we've seen that that is not always beneficial. So, a natural question is whether we can achieve the same (or better?) model performance by using only a subset of the columns. \n",
    "\n",
    "\n",
    "It's easy to train a model on a subset of the data. For example: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training with columns ['Pclass', 'Sex', 'Age', 'Siblings/Spouses Aboard', 'Parents/Children Aboard']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8025072420337629"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#all the columns except fare\n",
    "cols = ['Pclass', 'Sex', 'Age', 'Siblings/Spouses Aboard', 'Parents/Children Aboard']\n",
    "print(\"training with columns \"+str(cols))\n",
    "\n",
    "LR=LogisticRegression()\n",
    "cross_val_score(LR,X_train[cols],y_train,cv=5).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interesting! Excluding the last column (Fare) actually improved our CV score slightly. \n",
    "\n",
    "## Systematic Feature Selection\n",
    "\n",
    "Now, let's write a function that will let us do this systematically. Our function will use cross-validation to avoid \"peeking\" at the test set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_column_score(cols):\n",
    "    \"\"\"\n",
    "    Trains and evaluates the model via crossvalidation on the columns\n",
    "    of the dataset with select indeces\n",
    "    \"\"\"\n",
    "    print(\"training with columns \"+str(cols))\n",
    "\n",
    "    LR=LogisticRegression()\n",
    "    return cross_val_score(LR,X_train[cols],y_train,cv=5).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training with columns ['Sex', 'Age', 'Fare']\n",
      "CV score is 0.773\n",
      "training with columns ['Pclass', 'Sex', 'Age']\n",
      "CV score is 0.795\n",
      "training with columns ['Pclass', 'Parents/Children Aboard']\n",
      "CV score is 0.671\n",
      "training with columns ['Pclass', 'Sex', 'Age', 'Siblings/Spouses Aboard', 'Parents/Children Aboard']\n",
      "CV score is 0.803\n",
      "training with columns ['Pclass', 'Sex', 'Age', 'Siblings/Spouses Aboard', 'Parents/Children Aboard', 'Fare']\n",
      "CV score is 0.794\n"
     ]
    }
   ],
   "source": [
    "combos = [['Sex', 'Age', 'Fare'],\n",
    "          ['Pclass', 'Sex', 'Age'],\n",
    "          ['Pclass', 'Parents/Children Aboard'],\n",
    "          ['Pclass', 'Sex', 'Age', 'Siblings/Spouses Aboard', 'Parents/Children Aboard'],\n",
    "          ['Pclass', 'Sex', 'Age', 'Siblings/Spouses Aboard', 'Parents/Children Aboard', 'Fare']]\n",
    "\n",
    "for combo in combos:\n",
    "    x=check_column_score(combo)\n",
    "    print(\"CV score is \"+str(np.round(x,3)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-  Which combo scored the best?\n",
    "-  Which combo scored the worst?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, lets check on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_column_score(cols):\n",
    "    \"\"\"\n",
    "    Test model performance on the data with selected columns\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"training with columns \"+ str(cols))\n",
    "    LR=LogisticRegression()\n",
    "    LR.fit(X_train[cols],y_train)\n",
    "    \n",
    "    return LR.score(X_test[cols],y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training with columns ['Sex', 'Age', 'Fare']\n",
      "The test score is 0.82\n",
      "training with columns ['Pclass', 'Sex', 'Age']\n",
      "The test score is 0.803\n",
      "training with columns ['Pclass', 'Parents/Children Aboard']\n",
      "The test score is 0.742\n",
      "training with columns ['Pclass', 'Sex', 'Age', 'Siblings/Spouses Aboard', 'Parents/Children Aboard']\n",
      "The test score is 0.831\n",
      "training with columns ['Pclass', 'Sex', 'Age', 'Siblings/Spouses Aboard', 'Parents/Children Aboard', 'Fare']\n",
      "The test score is 0.82\n"
     ]
    }
   ],
   "source": [
    "for combo in combos:\n",
    "    x=test_column_score(combo)\n",
    "    print(\"The test score is \"+ str(np.round(x,3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Indeed, we achieved a higher prediction score on the test set by ignoring the \"Fare\" column completely. \n",
    "\n",
    "There are a number of sophisticated algorithms for automated feature selection, but we won't go further into this topic in this course. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
