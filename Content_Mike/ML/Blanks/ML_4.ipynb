{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overfitting I\n",
    "​\n",
    "Major parts of this lecture are borrowed from the discussion of [Hyperparameters and Model Validation](https://jakevdp.github.io/PythonDataScienceHandbook/05.03-hyperparameters-and-model-validation.html) Jake VanderPlas's excellent online book, the [Python Data Science Handbook](https://jakevdp.github.io/PythonDataScienceHandbook/index.html). \n",
    "​\n",
    "In the last lecture, we saw that a reasonably simple decision tree was able to achieve almost 80\\% accuracy in modeling from demographic data whether or not a given passenger survived the Titanic. However, we shouldn't take this result at face value. To see why, let's take a look at an example, in the context of regression (i.e. predicting a quantitative variable). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#standard imports\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's generate some random data of the form y=log(x) plus a random error.\n",
    "\n",
    "First we seed the generator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, lets make the data, where X is ten randomly chosen points between 0 and 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "def PolynomialRegression(degree=2, **kwargs):\n",
    "    return make_pipeline(PolynomialFeatures(degree),\n",
    "                         LinearRegression(**kwargs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The return value of `PolynomialRegression()` is a model, which we can then fit. There is an unfortunate syntactic requirement here -- we need to transform `X`, which is currently a 1d array, into a 2d array in which the second dimension has length 1.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's it for model fitting! Let's check quantitatively how we did: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The way that the score is computed depends on the model, but for this class of models the highest possible value of the score is 1. So, `model2` achieves essentially a perfect score, whereas `model1` achieves a noticeably lower score. \n",
    "\n",
    "Let's visualize to see how `model2` achieved such an impressive score. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The curvey red line perfectly fits the data! Machine learning is easy.\n",
    "\n",
    "But... does this make any sense???? The red squiggle looks nothing like a logarithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the red line is __great__ at 10 select points, but __terrible__ everywhere else"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's look at how the models perform on unseen data from a computational prespective"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#new data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#newscores\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Woops! This time, `model1`'s score is similar to what it achieved previously, but `model2`'s score is actually negative-very negative. The useless model that just guesses `0` for each prediction would have yielded a score of `0`, so `model2` is literally worse than useless. We can see this when we visualize: `model2` perfectly fits the original points, but is often badly off on the new ones. In contrast, `model1` is pretty close. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overfitting\n",
    "\n",
    "*Overfitting* refers to the phenomenon in which a machine learning model fits the *random noise* in the data, rather than the *true signal*. A characteristic sign of overfitting is that the model performs much less well on unseen data than on the data to which it was fit. In our case, we would say that `model2` is badly overfit -- it focused so much on the random wiggles in the data that it completely missed the overall upward trend. As a result, it cannot be used for prediction. In contrast, `model1` did ok -- it missed the fact that the true signal in the data is curved, but it can still be used to generate fairly reasonable predictions. \n",
    "\n",
    "Another aspect of overfitting is *model flexibility*. The less flexible `model1` didn't have enough flexibility to fit the curved pattern of the data, but it also didn't have enough flexibility to get distracted by the random noise. In contrast, the more flexible `model2` had so much flexibility that it was able to perfectly fit the noise in the data. This interpretation suggests that we should seek models with intermediate degrees of flexibility. In the next lecture, we'll discuss how to use model validation techniques to do this, and thereby diagnose and avoid overfitting. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
