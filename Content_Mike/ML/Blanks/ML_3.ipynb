{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to ML in Practice\n",
    "\n",
    "In the previous lecture, we worked through an example of machine learning \"by hand,\" in a very abstract mathematical context. All of the data was given to us as floating point numbers, and there was no messy data preparation to do. This enabled us to go into some detail about what the model was actually doing. We worked with a model with several parameters; generated instances of that model; and saw that selecting the model with the smallest loss led to a good apparent match to the data. \n",
    "\n",
    "In practice, the situation is not nearly so neat. There is missing data; some of the data has `na` values; some of the data is formatted as strings. Furthermore, it's arduous to do all of the steps of the machine learning pipeline by hand, and only machine learning researchers do this on a regular basis. Most programming languages offer simplified interfaces for efficiently working with machine learning models. In this lecture, we'll work through a similar machine learning pipeline on a more complex historical data set. Along the way, we'll introduce the `scikit-learn` package, which offers an effective and concise set of tools for carrying out complicated machine learning tasks. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To help us get acquainted with the many tools that `scikit-learn` offers us, we are going to work through the \"Hello World\" of machine learning -- the Titanic data set.\n",
    "\n",
    "__Before we get started:__ make sure that the file titanic.csv is in the same folder as this notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Titanic data set collects information about almost 900 passangers aboard the Titanic during the fateful voyage when it crashed into an iceberg in 1912 and sank. The information includes their age; the fare they paid for their ticket (in British pounds); their sex; and the passenger class Pclass, with 1st class corresponding to VIP treatment and 3rd class corresponding to a much less luxurious experience. Crucially, the data set also records whether that passenger survived the sinking of the ship, with 1 indicating that the passenger survived and 0 indicating that the passenger tragically perished.\n",
    "\n",
    "We are eventually going to train an algorithm to predict whether a passenger survived the Titanic based on their available information. Before we do, let's get a sense for some trends using familiar pandas summarization.\n",
    "\n",
    "How wealthy were these passengers? We can't know for certain, but we can get a sense for how much was paid for each passenger class.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#average / len of fare by Pclass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The average price of 84 pounds for a first-class ticket corresponds to nearly \\$15,000 USD today.\n",
    "- The second-class ticket corresponds to roughly \\$3,500\n",
    "- The third class ticket corresponds to roughly \\$2,500. \n",
    "\n",
    "We can safely assume that the first-class passengers were indeed substantially more wealthy on average than the others. \n",
    "\n",
    "As we shall see below, this difference in wealth made a considerable difference in how likely passengers were to survive. Moreover, we can also see that sex differences played an import role. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#survival by PClass and Sex"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wealthy females had a 97% chance of surviving, whereas poor males had a 14%.\n",
    "\n",
    "This table reflects the famous maritime tradition of prioritizing women and children first into the lifeboats, resulting in vastly higher survival rates among women in these data. Note the role of class: a 1st-class woman was twice as likely to survive as a third class woman, and a 1st-class man was nearly three times as likely to survive as a 3rd class man. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Towards Modeling\n",
    "\n",
    "We'd like to develop automated models that can use these trends and others to make predictions about survival. However, we need to do a bit of data cleaning before we're ready for this. In particular, **machine learning algorithms don't really get text**, so we need to transform text data into numbers before we can proceed. \n",
    "\n",
    "To encode the `Sex` data, we use the LabelEncoder function found within the preprocessing submodule of sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have defined our LabelEncoder, it will do all the hardwork for us"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also drop the name column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This looks better! We have a data frame full of sweet, delicious numbers. Mmmmm.....\n",
    "\n",
    "The next thing to do is to separate out the target data `Survived` from the predictor data (everything else). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, lets look at X and y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling\n",
    "\n",
    "To use a machine learning model from `scikit-learn`, you should import the relevant model. For example, let's use a decision tree classifier. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In `scikit-learn`, abstract models are represented as classes. To make a specific model, instantiate the class. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Arguments passed to the model upon instantiation are typically used to control how complex the model can be. These arguments are often referred to as *hyperparameters*. In practice, we don't usually know what the *right* hyperparameters are, and so we need to resort to various computational techniques (in coming lectures) to select good ones. \n",
    "\n",
    "Now, here's the good news. Once we've carefully prepared our data and instantiated a model, actually fitting the model is unreasonably easy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fit T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember that part in the last lecture where we defined a loss function and then searched around for models in our specified model family that minimized the loss? Yeah, `scikit-learn` has taken care of all that for us. \n",
    "\n",
    "We can then *score* our model to see how we did in terms of accuracy. The `score` method requires both the predictor and target variables. \n",
    "\n",
    "What does the `score` function calculate? It depends by model, but in the case of classifiers, the score is the fraction of the time that the model made the correct prediction. The score does the same job as the <font color=\"red\">loss function</font>, but the signs are flipped -- high scores mean good models. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hey, not bad! Our model was able to use the predictor variables to be right nearly 80\\% of the time. That's pretty impressive, but there is an important problem here. When doing machine learning, it's not advised to *score* or *evaluate* your model on the same data used for *training* or *fitting* the model. We'll come back to this in the next lecture. \n",
    "\n",
    "## Interpretation\n",
    "\n",
    "In most cases, you should aim to understand how your model makes the decisions that it does. This is the problem of machine learning *interpretation*. \n",
    "\n",
    "Every machine learning algorithm has both strengths and weaknesses when it comes to interpretation. Decision trees are pretty pleasant to interpret, as they correspond to \"flow-chart\" style reasoning that many of us are familiar with. The `tree` module of `scikit-learn` provides a convenient method for visualizing decision trees. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax=plt.subplots(1,figsize=(10,10))\n",
    "\n",
    "p=tree.plot_tree(T,\n",
    "                 filled=True,\n",
    "                 feature_names=X.columns\n",
    "                )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The automated decision tree classifier has found the following rule to predict whether a passenger survived the Ttianic. First, check whether `Sex <= 0.5` -- that is, check whether the passenger is female. \n",
    "- If so, next check whether the passenger was first-class. If so, then they survive with high probability (161/170); otherwise the algorithm isn't sure and gives them a roughly 50-50 chance. \n",
    "- If the passenger is male, then check how old they are. If they are younger than 13 years old, the algorithm gives them a fair chance of survival (remember: \"women and children.\"). Otherwise, the algorithm gives them very low odds indeed (86/532). \n",
    "\n",
    "We've now gone through one cycle of: \n",
    "\n",
    "1. Acquiring data.\n",
    "2. Exploratory analysis.\n",
    "3. Modeling. \n",
    "4. Interpreting results.\n",
    "\n",
    "In practice, we would not stop here. Having trained our model, we should then ask: \n",
    "\n",
    "- What new insights can we gain from our model about the underlying data set? \n",
    "- Can we improve our model?\n",
    "- How can we use what we have learned in other data sets?\n",
    "\n",
    "These questions, and many others, are part of the *cycle of data science*: \n",
    "\n",
    "<figure class=\"image\" style=\"width:100%\">\n",
    "  <img src=\"https://d33wubrfki0l68.cloudfront.net/795c039ba2520455d833b4034befc8cf360a70ba/558a5/diagrams/data-science-explore.png\" alt=\"A diagram with the words import, tidy, transform, visualize, model, and communicate. Import points to tidy. Tidy points to transform. Transform points to visualize, visualize poitns to model, and model points to visualize, thus forming a directed cycle of arrows. This cycle collectively points to the final word, communicate.\" width=\"600px\">\n",
    "    <br>\n",
    "    <caption><i>The cycle of data science. Image credit: Hadley Wickham</i></caption>\n",
    "</figure>\n",
    "\n",
    "We've learned to: \n",
    "\n",
    "- Import data (`pd.read_csv()`)\n",
    "- Tidy data (preprocessing)\n",
    "- Explore data by\n",
    "    - Transforming (e.g. creating new columns)\n",
    "    - Visualizing (`matplotlib`)\n",
    "    - Modeling (basic machine learning)\n",
    "    \n",
    "In your project, you'll be expected to go around this cycle multiple times, progressively building up your understanding and *communicating* your results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
