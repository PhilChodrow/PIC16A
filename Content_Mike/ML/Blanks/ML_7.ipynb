{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Selection\n",
    "\n",
    "In the last few lectures, we learned how to use hold-out \"test\" sets and cross-validation to gain appropriate estimates of a model's performance on unseen data. There, the focus was on choosing a good \"complexity\" parameter, such as the depth of a decision tree. In this lecture, we'll instead show how to use cross-validation to get an estimate of which columns in the data should or should not be included in a model. It's very common in practice that not all columns will be used in the best model, and many, many machine learning reseachers devote their careers to studying the problem of how to intelligently and automatically choose only the most relevant columns for models. In the literature, this problem is usually called *feature selection*. In this lecture, we'll take a quick look at how feature selection can improve model performance. \n",
    "\n",
    "For this demonstration, we'll switch from decision trees to logistic regression. Logistic regression is a form of regression modeling well-suited for predicting probabilities and class labels. \n",
    "\n",
    "Let's begin by running some familiar blocks of code, in which we load our core libraries, read in the data, split the data, and clean the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#standard imports\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "#read in data\n",
    "titanic=pd.read_csv(\"titanic.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to divide the data into a training and testing sets and prepocess it by changing male/female to 1/0 and dropping the name column. This code is nearly exactly the same as earlier videos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "np.random.seed(1111)\n",
    "train, test = train_test_split(titanic, test_size = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "def prep_titanic_data(data_df):\n",
    "    df = data_df.copy()\n",
    "    le = preprocessing.LabelEncoder()\n",
    "    df['Sex'] = le.fit_transform(df['Sex'])\n",
    "    df = df.drop(['Name'], axis = 1)\n",
    "    \n",
    "    X = df.drop(['Survived'], axis = 1)\n",
    "    y = df['Survived']\n",
    "        \n",
    "    return(X, y)\n",
    "\n",
    "X_train, y_train = prep_titanic_data(train)\n",
    "X_test,  y_test  = prep_titanic_data(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using logistic regression is almost exactly the same as using the decision tree classifier. Let's go ahead and use cross-validation to estimate the predictive performance of the model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Is this the best we can do? If you've studied logistic regression before, you may know that using lots of columns doesn't always help -- due to *multicollinearity*, the model's predictive performance can actually suffer. This is actually another aspect of *overfitting*. Adding more columns makes the model more flexible, and we've seen that that is not always beneficial. So, a natural question is whether we can achieve the same (or better?) model performance by using only a subset of the columns. \n",
    "\n",
    "\n",
    "It's easy to train a model on a subset of the data. For example: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#all the columns except fare\n",
    "cols = ['Pclass', 'Sex', 'Age', 'Siblings/Spouses Aboard', 'Parents/Children Aboard']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interesting! Excluding the last column (Fare) actually improved our CV score slightly. \n",
    "\n",
    "## Systematic Feature Selection\n",
    "\n",
    "Now, let's write a function that will let us do this systematically. Our function will use cross-validation to avoid \"peeking\" at the test set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "combos = [['Sex', 'Age', 'Fare'],\n",
    "          ['Pclass', 'Sex', 'Age'],\n",
    "          ['Pclass', 'Parents/Children Aboard'],\n",
    "          ['Pclass', 'Sex', 'Age', 'Siblings/Spouses Aboard', 'Parents/Children Aboard'],\n",
    "          ['Pclass', 'Sex', 'Age', 'Siblings/Spouses Aboard', 'Parents/Children Aboard', 'Fare']]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-  Which combo scored the best?\n",
    "-  Which combo scored the worst?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, lets check on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Indeed, we achieved a higher prediction score on the test set by ignoring the \"Fare\" column completely. \n",
    "\n",
    "There are a number of sophisticated algorithms for automated feature selection, but we won't go further into this topic in this course. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
