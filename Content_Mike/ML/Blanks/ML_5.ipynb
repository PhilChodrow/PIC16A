{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overfitting II\n",
    "\n",
    "Last time, we saw a theoretical example of *overfitting*, in which we fit a machine learning model that perfectly fit the data it saw, but performed extremely poorly on fresh, unseen data. In this lecture, we'll observe overfitting in a more practical context, using the Titanic data set again. We'll then begin to study *validation* techniques for finding models with \"just the right amount\" of flexibility. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Standard imports"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Before we get started:__ make sure the file titanic.csv is in the same folder as this notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read in data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Before:__ We diagnosed overfitting by testing our model against some new data.\n",
    "\n",
    "__Here:__ We don't have any more data. \n",
    "\n",
    "__Solution:__ Hold out some data that we won't let our model see at first. This holdout data is called the *validation* or *testing* data, depending on the use to which we put it. In contrast, the data that we allow our model to see is called the *training* data.\n",
    "\n",
    "`sklearn.model_selection` provides a convenient function for partitioning our data into training and holdout sets called `train_test_split`. The default and generally most useful behavior is to randomly select rows of the data frame to be in each set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have two data frames. As you may recall from a previous lecture, we need to do some data cleaning, and split them into predictor variables `X` and target variables `y`. Since we are going to preprocess two data frames, let's write a function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's apply this function to both dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Now we're able to train our model on the `train` data, and then evaluate its performance on the `val` data. This will help us to diagnose and avoid overfitting.\n",
    "\n",
    "Let's try using the decision tree classifier again. As you may remember, the `DecisionTreeClassifier()` class takes an argument `max_depth` that governs how many layers of decisions the tree is allowed to make. Larger `max_depth` values correspond to more complicated trees. In this way, `max_depth` is a model complexity parameter, similar to the `degree` when we did polynomial regression. \n",
    "\n",
    "For example, with a small `max_depth`, the model scores on the training and validation data are relatively close. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's try it with a larger max_depth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model with max depth too large overfits the data. Let's flesh this out in a bit more detail."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "N=30 #largest max_depth\n",
    "training_scores=np.zeros(N)\n",
    "testing_scores=np.zeros(N)\n",
    "depths=np.arange(1,31)\n",
    "\n",
    "\n",
    "for d in range(1, 30):\n",
    "    T = tree.DecisionTreeClassifier(max_depth = d)\n",
    "    T.fit(X_train, y_train)\n",
    "    training_scores[d-1]=T.score(X_train,y_train)\n",
    "    testing_scores[d-1]=T.score(X_test,y_test)\n",
    "\n",
    "fig, ax = plt.subplots(1, figsize = (10, 7))\n",
    "ax.scatter(depths,training_scores,color=\"black\",label=\"training\")\n",
    "ax.scatter(depths,testing_scores,color=\"firebrick\",label=\"testing\")    \n",
    "ax.set(xlabel = \"Complexity (depth)\", ylabel = \"Performance (score)\",ylim=(.73,1))\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observe that the training score (black) always increases, while the test score (red) tops out around 83\\% and then even begins to trail off slightly. It looks like the optimal depth might be around 5-7 or so, but there's some random noise that can prevent us from being able to determine exactly what the optimal depth is. \n",
    "\n",
    "Increasing performance on the training set combined with decreasing performance on the test set is the trademark of overfitting. \n",
    "\n",
    "This noise reflects the fact that we took a single, random subset of the data for testing. In a more systematic experiment, we would draw many different subsets of the data for each value of depth and average over them. This is what *cross-validation* does, and we'll talk about it in the next lecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
