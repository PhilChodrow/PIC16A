{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering\n",
    "\n",
    "So far in this course, we've focused our attention on __supervised__ machine learning, and in particular, on two fundamental tasks. \n",
    "\n",
    "- **Regression** aims to predict the value of a quantitative variable. \n",
    "- **Classification** aims to predict the value of a qualitative variable. \n",
    "\n",
    "However, this isn't all there is to machine learning. In this lecture, we're going to take a quick look at another task, called __clustering.__ Clustering fits into the broad set of __unsupervised__ machine learning tasks. In unsupervised tasks, there's no target variable to predict, and therefore no \"right value.\" Instead, the aim of an unsupervised algorithm is to explore the data and detect some latent structure. \n",
    "\n",
    "Clustering is the most common example of unsupervised tasks. In a clustering task, we hypothesize that the data may be naturally divided into dense clusters. The purpose of a clustering algorithm is to find these clusters. \n",
    "\n",
    "This lecture is based on the chapter [*In Depth: k-Means Clustering*](https://jakevdp.github.io/PythonDataScienceHandbook/05.11-k-means.html) of the [*Python Data Science Handbook*](https://jakevdp.github.io/PythonDataScienceHandbook/) by Jake VanderPlas. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#standard imports\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by generating some synthetic data. The `make_blobs()` function will create a user-specified number of \"blobs\" of data, each of which are reasonably well-separated from each other. Under the hood, it does this by assigning a true label to each data point, which it then returns as `y_true`. However, in a standard clustering task, we would not assume that the true labels exist, and we won't use them here. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look. Remember X is a two-dimensional numpy array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visually, it appears that there are 4 clusters.\n",
    "\n",
    "In real-world data, 'eye-balling' it isn't always a viable option. (Often real-world data is very high-dimensional.) Therefore, we should try to come up with an algorithmic approach to finding the clusters. There are many great algorithms for this, the famous of which is KMeans clustering. (K is the number of clusters.)\n",
    "\n",
    "Let's import `KMeans` and see how we do: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get cluster labels, we use the predict() method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's visualize the results. The use of the c and cmap arguments of ax.scatter() allow us to easily plot points of multiple colors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like `k-means` did a pretty good job of detecting our clusters! \n",
    "\n",
    "Under the hood, `k-means` tries to identify a \"centroid\" for each cluster. The two main principles are: \n",
    "\n",
    "1. Each centroid is the mean of all the points to which it corresponds. \n",
    "2. Each point is closer to its centroid than to any other centroid. \n",
    "\n",
    "The `KMeans` class makes it easy to retrieve the cluster centroids and visualize them. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the cluster centroids do indeed correspond pretty nicely to the \"middle\" of each of the identified clusters. \n",
    "\n",
    "This experiment went very well, but of course, things in the real world aren't that easy. Let's take a look at the Palmer Penguins again, for example. __You have my permission to stop typing here__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "penguins=pd.read_csv(\"palmer_penguins.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's make a scatter plot of culmen lengths and depths by species"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax=plt.subplots(1)\n",
    "\n",
    "for species in penguins['Species'].unique():\n",
    "    df=penguins[penguins['Species']==species]\n",
    "    ax.scatter(df['Culmen Length (mm)'],\n",
    "               df['Culmen Depth (mm)'],\n",
    "               label=species)\n",
    "ax.legend()    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we include the colors, it looks like there might be some clusters of penguins here. Maybe even 3? Let's see. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=penguins[[\"Culmen Length (mm)\", \"Culmen Depth (mm)\"]].dropna()\n",
    "\n",
    "kmeans=KMeans(n_clusters=3)\n",
    "kmeans.fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax=plt.subplots(1)\n",
    "ax.scatter(X[\"Culmen Length (mm)\"], X[\"Culmen Depth (mm)\"],\n",
    "           c=kmeans.predict(X));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ummmm...these don't really seem like the \"right\" clusters! There's a reason that `k-means` doesn't do very well on the Penguins data set. `k-means` works best when the clusters are roughly *circular*. On the other hand, the true clusters in the penguins data set appear to be elliptical (stretched out). They also have a somewhat diagonal direction, not aligned with the horizontal or vertical axis. There are more sophisticated clustering algorithms that can handle data with this kind of structure, but we won't focus on them in this class. \n",
    "\n",
    "For a nice exposition of a more sophisticated clustering algorithm (with helpful diagrams), you might want to check out Jake VanderPlas's [chapter on Gaussian mixture models](https://jakevdp.github.io/PythonDataScienceHandbook/05.12-gaussian-mixtures.html) in the [*Python Data Science Handbook*](https://jakevdp.github.io/PythonDataScienceHandbook/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
