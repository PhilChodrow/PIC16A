{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning \"By Hand\"\n",
    "\n",
    "Recall from the previous lecture that the supervised machine learning task has four major components: \n",
    "\n",
    "1. The <font color=\"green\"><i>predictor variables</i> $X$</font>. \n",
    "2. The <font color=\"gold\"><i>target variable</i> $Y$</font>, which we aim to predict using <font color=\"green\">$X$</font>. \n",
    "3. The <font color=\"blue\"> <i>model</i> $f$ </font>. We treat $\\color{blue}{f}(\\color{green}{X})$ as our estimate of $\\color{gold}{Y}$. \n",
    "1. <font color=\"red\"><i>The loss function</i> $\\mathcal{L}$</font>. The quantity $\\color{red}{\\mathcal{L}}(\\color{blue}{f}(\\color{green}{X}), \\color{gold}{Y})$ is a measure of how well the model $\\color{blue}{f}$ \"fits\" the data $(\\color{green}{X}, \\color{gold}{Y})$. \n",
    "\n",
    "In this lecture, we will explore each of these components in an interpretable setting -- linear regression. This will help us understand what's really going on when we start using more complicated models from Python packages. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression\n",
    "\n",
    "In linear regression, we use a *linear* model for the data. In the 1-dimensional case, this means that our model $\\color{blue}{f}$ has the form \n",
    "\n",
    "$$\\color{blue}{f}(\\color{green}{x}) = a\\color{green}{x}+b \\approx \\color{gold}{y}\\;.$$\n",
    "\n",
    "There are two parameters: the slope $a$ and the intercept $b$. By changing the slope and intercept, we get different models. We say that $\\color{blue}{f}$ belongs to a *family* of models $\\color{blue}{\\mathcal{M}}$, with each model corresponding to a different choice of $a$ and $b$. Our learning task now is to find good choices for $a$ and $b$, given some data. \n",
    "\n",
    "## Predictor and Target Data\n",
    "\n",
    "Let's now generate some synthetic data to use as our example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#standard imports\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use a synthetic, randomly generated data set The following line seed our random number generator so we get the same thing every time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our data will be 100 points from the line y=x+1 plus a random error which we model as a normal random variable with standard deviation .2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at our data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot points and true function\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In practice, we don't actually know the true function. So our situtaion looks a bit more like this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we stare at the data, we can see that it kind of looks like we might be able to fit it with a line of the form \n",
    "\n",
    "Y=a*X+b\n",
    "\n",
    "Let's use a function to formalize this model. When thinking about this conceptually we can regard X as the input and regard a and b as learnable parameters, but when we write the code they are all just input parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#LM stands for linear model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how our model works against some data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some of these <font color = \"blue\"> models </font> look better than others! How do we pick, systematically? Well, that's where the <font color=\"red\"> loss function $\\mathcal{L}$ </font> comes in. The most common choice in linear regression is the <font color=\"red\"> <i> mean-square error</i></font>, which is defined as follows: \n",
    "\n",
    "$$\\color{red}{\\mathcal{L}}(\\color{blue}{f}(\\color{green}{X}),\\color{gold}{Y}) = \\frac{1}{n}\\left[ (\\color{gold}{y}_1 - \\color{blue}{f}(\\color{green}{x}_1))^2 + (\\color{gold}{y}_2 - \\color{blue}{f}(\\color{green}{x}_2))^2 + \\cdots + (\\color{gold}{y}_n - \\color{blue}{f}(\\color{green}{x}_n))^2\\right]$$\n",
    "\n",
    "A term like $(\\color{gold}{y}_i - \\color{blue}{f}(\\color{green}{x}_i))^2$ is large when $\\color{blue}{f}(\\color{green}{x}_i)$ is very different from $\\color{gold}{y}_i$ -- that is, when our prediction is off! So, if a <font color=\"blue\">model</font> has a low <font color=\"red\">mean-square error </font>$\\color{red}{\\mathcal{L}}$, then this indicates that the <font color=\"blue\">model</font> \"fits the data\" well. \n",
    "\n",
    "Let's implement the <font color=\"red\">mean-square error</font> for linear regression.  The error depends on the parameters $a$ and $b$. `numpy` array operations make this very easy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loss function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's go back to our plot of the data, and show how all those candidate <font>models</font> fare with regards to the <font color=\"red\">MSE loss function</font>. We're going to \n",
    "\n",
    "-  Make all of our lines black\n",
    "-  tune our visualization so that the <font color=\"blue\">models</font> with lower <font color=\"red\">MSE</font> are drawn thicker:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hey, this looks pretty good! The <font color=\"blue\">models</font> that have lower <font color=\"red\">MSE</font> (darker lines) \"look close\" to the data. \n",
    "\n",
    "Let's see if we can estimate $a$ and $b$. One way to do this is by simply generating a lot of random possibilities and picking the best one. Let's plot a number of models and highlight the best one in a different color."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot data points\n",
    "\n",
    "#make 100 lines plot them, if error<best error perform update\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, lets check on the best_a and best_b __learned__ from the data. (Recall the true values are a=b=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recap\n",
    "\n",
    "In this lecture, we did linear regression \"by hand.\" We generated some synthetic <font color=\"green\">predictor data</font> and <font color=\"gold\">target data</font>. We then modeled the data using a family of one-dimensional <font color=\"blue\"> linear models</font>, and selected from among the many possibilities using the <font color=\"red\">mean square error loss function</font>. Choosing the <font color=\"blue\">model </font> that minimized the <font color=\"red\">loss function</font> led to a good \"fit\" to the data. \n",
    "\n",
    "This pattern applies to essentially all problems in (supervised) machine learning: \n",
    "\n",
    "1. Collect some <font color=\"green\">predictor data</font> and <font color=\"gold\">target data</font>. \n",
    "2. Define a family of <font color=\"blue\">models</font> and <font color=\"red\">loss function</font>.  \n",
    "3. Find the element of the <font color=\"blue\">model family</font> that minimizes the <font color=\"red\">loss function</font>. \n",
    "\n",
    "There are a few outstanding issues that we haven't covered here. The biggest one is that \"fitting the data\" is not actually what we usually care about -- we care about *predicting* unseen data. It turns out that fitting the data too closely can actually be counter productive in this case. This is the problem of *overfitting*, which we'll consider in a future lecture. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus material\n",
    "In the above example, we just picked a bunch of different a's and b's completely at random. We did this, because it is simple, but it is not what you would do in practice. Without getting to much into the weeds, here is what you would do in practice.\n",
    "\n",
    "### Conceptual - Gradient Descent\n",
    "\n",
    "1) Start with a initial guesses a_0, b_0.\n",
    "\n",
    "2) Based on how your model works with a_0 and b_0, come up with another guess a_1 and b_1 which are better than the initial guess\n",
    "\n",
    "3) Repeat this processs so that a_2,b_2 are better than a_1,b_1, and a_3,b_3 are even better than a_2,b_2,...\n",
    "\n",
    "4) Don't stop until your model is sufficiently good\n",
    "\n",
    "If you have taken calculus (not required) this might remind you of Newton's method for solving equations. Indeed, the rule for getting a \"better guess\" relies on taking the derivative of the loss function and doing some calculus. The name for this procedure is Gradient Descent.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Practical - Scipy.optimize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.optimize import minimize\n",
    "res = minimize(lambda z: linear_MSE(X, Y, z[0], z[1]), np.array([0,0]))\n",
    "best_a, best_b = res.x\n",
    "best_a, best_b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is exactly what is going on \"under the hood\" of most prepackaged machine learning algorithms, which we'll begin to see in the next few lectures. \n",
    "\n",
    "Having obtained the optimal parameters, we are now able to make predictions on unseen data. For example: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "LM(np.array([0.7]), best_a, best_b) # model prediction when X = 0.7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
